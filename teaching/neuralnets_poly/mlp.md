# Perceptron multi-couches

<http://cs231n.github.io/neural-networks-1/>

## Cas d'un réseau de neurones à une couche cachée

### Calcul de gradient

### Visualisation de frontières apprises

Exemple intéressant : <http://cs231n.github.io/neural-networks-case-study/>

[comment]: # (Montrer que l'augmentation du nombre de neurones dans la couche permet d'apprendre des frontières plus complexes.)
[comment]: # (Finir par le théorème (?) qui dit que c'est mieux d'avoir plus de couches que juste une couche avec pleins de neurones.)

## Cas multi-couches

### Calcul de gradient

<http://cs231n.github.io/optimization-2/>

[comment]: # (Montrer pourquoi faire du backprop pour des raisons de complexité)

### Visualisation

Comprendre qu'on apprend une représentation (eg PCA améliorée) suivie d'un classifieur linéaire (logistic regression)

* <https://rajarsheem.wordpress.com/2017/05/04/neural-networks-dynamics/>
* <http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/>

### Exemples adversaires

## Quelques considérations pratiques

### Initialisation des poids du réseau

[comment]: # (Approche historique (random).)
[comment]: # (Approche _deep learning_ : _Stacked Auto-encoders_.)

### Régularisation

<http://cs231n.github.io/neural-networks-2/>

#### Régularisations L1/L2

#### Utilisation du _dropout_
